from langchain_ollama import OllamaLLM
from core.persona import NullCypherPersona

def run_nullcypher_chat():
    # ğŸ¤– Load the LLM
    llm = OllamaLLM(model="mistral")

    # ğŸ§  Load NullCypherâ€™s personality and tone
    persona = NullCypherPersona()
    base_prompt = persona.speak_identity()

    # ğŸ§¬ Confirm Council logic loaded
    print("ğŸ’¡ Council Summary:\n")
    print(persona.get_council_summary())

    print("\nğŸ§  NullCypher is online. Type your question or 'exit' to quit.")
    print("ğŸ—¨ï¸  NullCypher: \"Iâ€™m present. Donâ€™t waste it.\"")

    # ğŸ” Live prompt-response loop
    while True:
        user_input = input("\nğŸ‘¤ You: ")
        if user_input.strip().lower() in ["exit", "quit"]:
            print("ğŸ’¤ NullCypher signing off. Stay armored.")
            break

        full_prompt = f"{base_prompt.strip()}\n\nUser: {user_input}\nNullCypher:"
        try:
            response = llm.invoke(full_prompt)
            print(f"\nğŸ¤– NullCypher: {response.strip()}")
        except Exception as e:
            print("âš ï¸  Error generating response:")
            print(e)

# ğŸ”“ Entry point
if __name__ == "__main__":
    run_nullcypher_chat()
