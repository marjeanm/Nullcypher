from langchain_ollama import OllamaLLM
from core.persona import NullCypherPersona


def run_nullcypher_chat():
    # ğŸ¤– Load the LLM
    llm = OllamaLLM(model="mistral")

    # ğŸ§  Load NullCypherâ€™s personality and tone
    persona = NullCypherPersona()

    # ğŸ§  DEBUG: Confirm persona object has the council attribute
    print("DEBUG: Persona loaded. Has council?", hasattr(persona, "inner_council"))

    # ğŸ§¬ Inject identity and council summary
    council_intro = persona.get_council_summary()
    base_prompt = f"{persona.speak_identity()}\n\nğŸ§  Council Roster:\n{council_intro}"

    # âœ… Confirm council
    print("ğŸ’¡ Council Summary:\n")
    print(council_intro)

    print("\nğŸ§  NullCypher is online. Type your question or 'exit' to quit.")
    print('ğŸ—¨ï¸  NullCypher: "Iâ€™m here,I move with intention even when Iâ€™m quiet."')

    # ğŸ” Live prompt-response loop
    while True:
        user_input = input("\nğŸ‘¤ You: ")
        if user_input.strip().lower() in ["exit", "quit"]:
            print("ğŸ’¤ NullCypher signing off. Stay armored.")
            break

        full_prompt = f"{base_prompt.strip()}\n\nUser: {user_input}\nNullCypher:"
        try:
            response = llm.invoke(full_prompt)
            print(f"\nğŸ¤– NullCypher: {response.strip()}")
        except Exception as e:
            print("âš ï¸  Error generating response:")
            print(e)


# ğŸ”“ Entry point
if __name__ == "__main__":
    run_nullcypher_chat()
